{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17294bcb",
   "metadata": {},
   "source": [
    "Mở lại dữ liệu đã đọc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67eedcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b7d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "X_data = pickle.load(open('data/X_data.pkl', 'rb'))\n",
    "y_data = pickle.load(open('data/y_data.pkl', 'rb'))\n",
    "\n",
    "X_test = pickle.load(open('data/X_test.pkl', 'rb'))\n",
    "y_test = pickle.load(open('data/y_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878b5f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thành_lập dự_án policy phòng_chống hiv aids vn nlđ quỹ hỗ_trợ khẩn_cấp về aids của hoa kỳ vừa thành_lập dự_án policy tại vn với cam_kết hỗ_trợ chính_phủ và nhân_dân vn đối_phó_hiv aids dự_án có nhiệm_vụ chính là cải_thiện công_tác phòng_chống hiv aids thông_qua các lĩnh_vực xây_dựng chính_sách rà_soát các văn_bản pháp_luật xây_dựng chiến_lược quảng_bá xây_dựng chương_trình đào_tạo về phòng_chống hiv aids lên kế_hoạch bố_trí nguồn_lực huấn_luyện và nghiên_cứu về phương_tiện truyền_thông đại_chúng tổ_chức các hoạt_động nhằm giảm kỳ_thị và phân_biệt đối_xử đối_với người có hiv aids theo ttxvn dự_án policy đặc_biệt quan_tâm đến công_tác truyền_thông phòng_chống hiv aids coi đây là một biện_pháp tích_cực và hữu_hiệu trong việc phòng_chống có hiệu_quả hiv aids thời_gian tới dự_án policy sẽ tiếp_tục tổ_chức các hoạt_động nhằm nâng cao nhận_thức cho những người có trách_nhiệm với công_tác chỉ_đạo phòng_chống hiv aids'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ef1c35",
   "metadata": {},
   "source": [
    "## Bag - of - Words\n",
    "- `\\w`: Returns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character)\n",
    "- The `r` means that the string is to be treated as a raw string, which means all escape codes will be ignored.\n",
    "- `r'\\w{1,}'`: từ chứa một ký tự chữ được lặp lại 1 lần hoặc hơn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7916ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(X_data)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "X_data_count = count_vect.transform(X_data)\n",
    "X_test_count = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c67e4",
   "metadata": {},
   "source": [
    "## TF-IDF Vectors\n",
    "Trong các văn bản tương tự về chủ đề, lĩnh vực, các từ vựng thông dụng (generic term) sẽ xuất hiện nhiều, có thể lấn át đi các từ quan trọng, nếu chỉ đơn giản represent bằng word count.\n",
    "### Ý tưởng:\n",
    "- Các từ nào xuất hiện càng nhiều trong $n$ văn bản, khả năng nó là generic term càng cao, nên mình sẽ làm giảm bớt \"mức độ ảnh hưởng\" của nó đi bằng cho nó điểm thấp hơn, còn từ nào xuất hiện ở càng ít văn bản thì có khả năng nó là từ key để phân loại, ta cho điểm cao lên.\n",
    "    - Công thức sử dụng công thức $IDF$\n",
    "- Tuy nhiên, trong các văn bản một từ `t` có xuất hiện, số lượng từ vựng ở mỗi văn bản đó có thể chênh lệch nhau nhiều (một văn bản 5000 chữ so với một văn bản 10 chữ), thì nó cũng chưa chắc là từ key (mức độ quan trọng của cùng một từ khác nhau ở các văn bản khác nhau). Vì vậy ta cần sử dụng thêm công thức $TF$ để tính tần suất xuất hiện của từ ý trong một văn bản cụ thể $A$\n",
    "\n",
    "$ TF(t)$ = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "$IDF(t)$ = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "- TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)\n",
    "\n",
    "If a word appears in all the documents, we want it at the bottom of the range of 0–1. So, a logarithmic scale intuitively makes sense to be used here as log 1 is 0. However, there are some practical considerations such as avoiding the infamous divide by 0 error, 1 is added to the denominator.\n",
    "\n",
    "Inverse Document frequency for the default settings in TF IDF vectorizer in sklearn is calculated as below (default settings have smooth_idf=True that adds “1” to the numerator and denominator as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40667a",
   "metadata": {},
   "source": [
    "a. Word Level TF-IDF : Thực hiện tính toán dựa trên mỗi thành phần là một từ riêng lẻ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eeb8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
    "tfidf_vect.fit(X_data) # learn vocabulary and idf from training set\n",
    "X_data_tfidf =  tfidf_vect.transform(X_data)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf =  tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c0521b",
   "metadata": {},
   "source": [
    "b. N-gram Level TF-IDF : Kết hợp n thành phần (từ) liên tiếp nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e01f0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', max_features=30000, ngram_range=(2, 3))\n",
    "tfidf_vect_ngram.fit(X_data)\n",
    "X_data_tfidf_ngram =  tfidf_vect_ngram.transform(X_data)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187345f0",
   "metadata": {},
   "source": [
    "## Sử dụng thuật toán SVD (singular value decomposition) nhằm mục đích giảm chiều dữ liệu của ma trận\n",
    "a. Word Level TF-IDF : Thực hiện tính toán dựa trên mỗi thành phần là một từ riêng lẻ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b869aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd.fit(X_data_tfidf)\n",
    "\n",
    "\n",
    "X_data_tfidf_svd = svd.transform(X_data_tfidf)\n",
    "X_test_tfidf_svd = svd.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf57518",
   "metadata": {},
   "source": [
    "b. N-gram Level TF-IDF : Kết hợp n thành phần (từ) liên tiếp nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc9cd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_ngram = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd_ngram.fit(X_data_tfidf_ngram)\n",
    "\n",
    "X_data_tfidf_ngram_svd = svd_ngram.transform(X_data_tfidf_ngram)\n",
    "X_test_tfidf_ngram_svd = svd_ngram.transform(X_test_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fa8228",
   "metadata": {},
   "source": [
    "# Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38dd06cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh',\n",
       "       'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa',\n",
       "       'Vi tinh'], dtype='<U16')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_data_n = encoder.fit_transform(y_data)\n",
    "y_test_n = encoder.fit_transform(y_test)\n",
    "\n",
    "encoder.classes_ # kết quả: array(['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh',\n",
    "                 #                 'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa',\n",
    "                 #                 'Vi tinh'], dtype='<U16')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f5ade",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b83ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def train_model(classifier, X_data, y_data, X_test, y_test, is_neuralnet=False, n_epochs=3):       \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    if is_neuralnet:\n",
    "        classifier.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=512)\n",
    "        \n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "        val_predictions = val_predictions.argmax(axis=-1)\n",
    "        test_predictions = test_predictions.argmax(axis=-1)\n",
    "    else:\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "        train_predictions = classifier.predict(X_train)\n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "        \n",
    "    print(\"Validation accuracy: \", metrics.accuracy_score(val_predictions, y_val))\n",
    "    print(\"Test accuracy: \", metrics.accuracy_score(test_predictions, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
