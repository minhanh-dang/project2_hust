{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd859646",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9facfe41",
   "metadata": {},
   "source": [
    "- The `sklearn` library contains a lot of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction\n",
    "- Keras is a high-level, deep learning API developed by Google for implementing neural networks\n",
    "- Pandas is a Python library used for working with data sets. It has functions for analyzing, cleaning, exploring, and manipulating data.\n",
    "- XGBoost is a popular and efficient open-source implementation of the gradient boosted trees algorithm\n",
    "- `pyvi`: Python Vietnamese Toolkit.\n",
    "- `tqdm`: used to create a smart progress bar for the loops\n",
    "- `numpy`: a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4080d12",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85182039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c74c91",
   "metadata": {},
   "source": [
    "## Diference between os.getcwd() and os.path.dirname(__file__)\n",
    "There is a difference, though you wouldn't be able to tell from a single script.\n",
    "\n",
    "`__file__` is the full filename of a loaded module or script, so getting the parent directory of it with `os.path.dirname(__file__)` gets you the directory that script is in.\n",
    "\n",
    "Note: on Linux (and similar OSes), such a filename can be a symbolic link to the actual file which may reside somewhere else. You can use `os.path.realpath()` to resolve through any such links, if needed, although you can typically use the symlink equivalently. On Windows these are less common, but similarly, you can resolve symbolic links through `realpath()`.\n",
    "\n",
    "`os.getcwd()` gets you the current working directory. If you start a script from the directory the script is in (which is common), the working directory will be the same as the result from the call from `os.path.dirname(__file__)`.\n",
    "\n",
    "But if you start the script from another directory (i.e. python `d:\\some\\path\\script.py`), or if you change the working directory during the script (e.g. with `os.chdir()`), the current working directory has changed, but the directory part of the script filename has not.\n",
    "\n",
    "So, it depends on what you need:\n",
    "- Need the directory your script file is in? Use `os.path.dirname(__file__)`\n",
    "- Need the directory your script is currently running in? use `os.getcwd()`\n",
    "\n",
    "You'll see / in some results and \\ in others. Sadly, MS Windows uses \\ to separate parts of a path (e.g. `C:\\Program Files\\App\\`), while pretty much all other operating systems use / (e.g. `/home/user/script.py`)\n",
    "\n",
    "Python will often convert those automatically, so you can use paths like `C:/Program Files/App` in Python on Windows as well, but it tends to be a good idea to be safe and use `os.path.sep`.\n",
    "\n",
    "Note: if you're on Python 3, you may be better off just using pathlib's Path instead of os.path. It automatically resolves symbolic links (although you can still resolve to the link if you prefer) and has other nice conveniences as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b046b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "dir_path = os.path.join(dir_path, 'project 2')\n",
    "# '/Users/macos/Desktop/Github/NLP/Text Classifier'\n",
    "# Load data from dataset folder\n",
    "# VNTC-master/Data/10Topics/Ver1.1/Train_Full\n",
    "# VNTC-master/Data/10Topics/Ver1.1/Test_Full\n",
    "def get_data(folder_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    dirs = os.listdir(folder_path)\n",
    "    for path in dirs:\n",
    "        file_paths = os.listdir(os.path.join(folder_path, path))\n",
    "        for file_path in tqdm(file_paths):\n",
    "            with open(os.path.join(folder_path, path, file_path), 'r', encoding=\"utf-16\") as f:\n",
    "                lines = f.readlines()\n",
    "                lines = ' '.join(lines)\n",
    "                lines = gensim.utils.simple_preprocess(lines)\n",
    "                lines = ' '.join(lines)\n",
    "                lines = ViTokenizer.tokenize(lines)\n",
    "#                 sentence = ' '.join(words)\n",
    "#                 print(lines)\n",
    "                X.append(lines)\n",
    "                y.append(path)\n",
    "#             break\n",
    "#         break\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf12cc0",
   "metadata": {},
   "source": [
    "> Chỉ chạy block code này một lần đầu để lưu đọc dữ liệu. Sau đó lưu vào file .pkl thì các lần sau chỉ việc lấy ra từ file đó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c19d44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5219/5219 [01:57<00:00, 44.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3159/3159 [01:23<00:00, 38.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1820/1820 [00:44<00:00, 40.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2552/2552 [00:57<00:00, 44.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3868/3868 [01:19<00:00, 48.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3384/3384 [01:13<00:00, 45.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2898/2898 [01:03<00:00, 45.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5298/5298 [02:11<00:00, 40.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3080/3080 [01:03<00:00, 48.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2481/2481 [00:40<00:00, 60.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 7567/7567 [02:13<00:00, 56.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2036/2036 [00:47<00:00, 43.25it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2096/2096 [00:41<00:00, 50.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5276/5276 [01:45<00:00, 50.11it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3788/3788 [01:07<00:00, 56.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5417/5417 [01:42<00:00, 52.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6716/6716 [01:56<00:00, 57.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6667/6667 [02:23<00:00, 46.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6250/6250 [02:12<00:00, 47.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 4560/4560 [01:22<00:00, 55.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_path = os.path.join(dir_path, 'VNTC-master/Data/10Topics/Ver1.1/Train_Full')\n",
    "# X_data, y_data = get_data(train_path)\n",
    "# test_path = os.path.join(dir_path, 'VNTC-master/Data/10Topics/Ver1.1/Test_Full')\n",
    "# X_test, y_test = get_data(test_path)\n",
    "# pickle.dump(X_data, open('data/X_data.pkl', 'wb'))\n",
    "# pickle.dump(y_data, open('data/y_data.pkl', 'wb'))\n",
    "\n",
    "# pickle.dump(X_test, open('data/X_test.pkl', 'wb'))\n",
    "# pickle.dump(y_test, open('data/y_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ee567",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c62abe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "X_data = pickle.load(open('data/X_data.pkl', 'rb'))\n",
    "y_data = pickle.load(open('data/y_data.pkl', 'rb'))\n",
    "\n",
    "X_test = pickle.load(open('data/X_test.pkl', 'rb'))\n",
    "y_test = pickle.load(open('data/y_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37925d7",
   "metadata": {},
   "source": [
    "## Bag - of - Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade9f640",
   "metadata": {},
   "source": [
    "- `\\w`: Returns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character)\n",
    "- The `r` means that the string is to be treated as a raw string, which means all escape codes will be ignored.\n",
    "- `r'\\w{1,}'`: từ chứa một ký tự chữ được lặp lại 1 lần hoặc hơn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eed7be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(X_data)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "X_data_count = count_vect.transform(X_data)\n",
    "X_test_count = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0378159d",
   "metadata": {},
   "source": [
    "## TF-IDF Vectors\n",
    "Trong các văn bản tương tự về chủ đề, lĩnh vực, các từ vựng thông dụng (generic term) sẽ xuất hiện nhiều, có thể lấn át đi các từ quan trọng, nếu chỉ đơn giản represent bằng word count.\n",
    "### Ý tưởng:\n",
    "- Các từ nào xuất hiện càng nhiều trong $n$ văn bản, khả năng nó là generic term càng cao, nên mình sẽ làm giảm bớt \"mức độ ảnh hưởng\" của nó đi bằng cho nó điểm thấp hơn, còn từ nào xuất hiện ở càng ít văn bản thì có khả năng nó là từ key để phân loại, ta cho điểm cao lên.\n",
    "    - Công thức sử dụng công thức $IDF$\n",
    "- Tuy nhiên, trong các văn bản một từ `t` có xuất hiện, số lượng từ vựng ở mỗi văn bản đó có thể chênh lệch nhau nhiều (một văn bản 5000 chữ so với một văn bản 10 chữ), thì nó cũng chưa chắc là từ key (mức độ quan trọng của cùng một từ khác nhau ở các văn bản khác nhau). Vì vậy ta cần sử dụng thêm công thức $TF$ để tính tần suất xuất hiện của từ ý trong một văn bản cụ thể $A$\n",
    "\n",
    "$ TF(t)$ = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "$IDF(t)$ = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "- TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)\n",
    "\n",
    "If a word appears in all the documents, we want it at the bottom of the range of 0–1. So, a logarithmic scale intuitively makes sense to be used here as log 1 is 0. However, there are some practical considerations such as avoiding the infamous divide by 0 error, 1 is added to the denominator.\n",
    "\n",
    "Inverse Document frequency for the default settings in TF IDF vectorizer in sklearn is calculated as below (default settings have smooth_idf=True that adds “1” to the numerator and denominator as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac95c5a9",
   "metadata": {},
   "source": [
    "a. Word Level TF-IDF : Thực hiện tính toán dựa trên mỗi thành phần là một từ riêng lẻ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65ebb6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
    "tfidf_vect.fit(X_data) # learn vocabulary and idf from training set\n",
    "X_data_tfidf =  tfidf_vect.transform(X_data)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf =  tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2dddf6",
   "metadata": {},
   "source": [
    "b. N-gram Level TF-IDF : Kết hợp n thành phần (từ) liên tiếp nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2329c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', max_features=30000, ngram_range=(2, 3))\n",
    "tfidf_vect_ngram.fit(X_data)\n",
    "X_data_tfidf_ngram =  tfidf_vect_ngram.transform(X_data)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343133a",
   "metadata": {},
   "source": [
    "c. Character Level TF-IDF : Dựa trên n-gram của ký tự."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d45ca74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram-char level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect_ngram_char = TfidfVectorizer(analyzer='char', max_features=30000, ngram_range=(2, 3))\n",
    "tfidf_vect_ngram_char.fit(X_data)\n",
    "X_data_tfidf_ngram_char =  tfidf_vect_ngram_char.transform(X_data)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf_ngram_char =  tfidf_vect_ngram_char.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21cfd4f",
   "metadata": {},
   "source": [
    "## Sử dụng thuật toán SVD (singular value decomposition) nhằm mục đích giảm chiều dữ liệu của ma trận"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fd854",
   "metadata": {},
   "source": [
    "a. Word Level TF-IDF : Thực hiện tính toán dựa trên mỗi thành phần là một từ riêng lẻ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6acbd6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd.fit(X_data_tfidf)\n",
    "\n",
    "\n",
    "X_data_tfidf_svd = svd.transform(X_data_tfidf)\n",
    "X_test_tfidf_svd = svd.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7a160",
   "metadata": {},
   "source": [
    "b. N-gram Level TF-IDF : Kết hợp n thành phần (từ) liên tiếp nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eede444",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_ngram = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd_ngram.fit(X_data_tfidf_ngram)\n",
    "\n",
    "X_data_tfidf_ngram_svd = svd_ngram.transform(X_data_tfidf_ngram)\n",
    "X_test_tfidf_ngram_svd = svd_ngram.transform(X_test_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5f261",
   "metadata": {},
   "source": [
    "c. Character Level TF-IDF : Dựa trên n-gram của ký tự."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51b3613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_ngram_char = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd_ngram_char.fit(X_data_tfidf_ngram_char)\n",
    "\n",
    "X_data_tfidf_ngram_char_svd = svd_ngram_char.transform(X_data_tfidf_ngram_char)\n",
    "X_test_tfidf_ngram_char_svd = svd_ngram_char.transform(X_test_tfidf_ngram_char)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c0188",
   "metadata": {},
   "source": [
    "# Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36271f6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh',\n",
       "       'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa',\n",
       "       'Vi tinh'], dtype='<U16')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_data_n = encoder.fit_transform(y_data)\n",
    "y_test_n = encoder.fit_transform(y_test)\n",
    "\n",
    "encoder.classes_ # kết quả: array(['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh',\n",
    "                 #                 'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa',\n",
    "                 #                 'Vi tinh'], dtype='<U16')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa841ec",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa81e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f81ccc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, X_data, y_data, X_test, y_test, is_neuralnet=False, n_epochs=3):       \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    if is_neuralnet:\n",
    "        classifier.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=512)\n",
    "        \n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "        val_predictions = val_predictions.argmax(axis=-1)\n",
    "        test_predictions = test_predictions.argmax(axis=-1)\n",
    "    else:\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "        train_predictions = classifier.predict(X_train)\n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "        \n",
    "    print(\"Validation accuracy: \", metrics.accuracy_score(val_predictions, y_val))\n",
    "    print(\"Test accuracy: \", metrics.accuracy_score(test_predictions, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a96afb",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4a630",
   "metadata": {},
   "source": [
    "Một giả thuyết rằng các chiều của dữ liệu $X= (x_1, x_2, x_3,\\dots,x_n)$ là độc lập về mặt xác suất với nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "792e1d21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.8524881516587678\n",
      "Test accuracy:  0.8628034859944812\n"
     ]
    }
   ],
   "source": [
    "train_model(naive_bayes.MultinomialNB(), X_data_tfidf, y_data, X_test_tfidf, y_test, is_neuralnet=False)\n",
    "\n",
    "# kết quả\n",
    "# Train accuracy:  0.880031596616529\n",
    "# Validation accuracy:  0.8690758293838863\n",
    "# Test accuracy:  0.8650666031405714\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece2aa7",
   "metadata": {},
   "source": [
    "Với dữ liệu đã được SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b58ba2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6316\\3767148002.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnaive_bayes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_data_tfidf_ngram_svd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_tfidf_ngram_svd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_neuralnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnaive_bayes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_data_tfidf_ngram_char_svd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_tfidf_ngram_char_svd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_neuralnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6316\\2288045952.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(classifier, X_data, y_data, X_test, y_test, is_neuralnet, n_epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtest_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mtrain_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m         \u001b[1;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m         \u001b[0mcheck_non_negative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    864\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1249\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "train_model(naive_bayes.MultinomialNB(), X_data_tfidf_ngram_svd, y_data, X_test_tfidf_ngram_svd, y_test, is_neuralnet=False)\n",
    "\n",
    "train_model(naive_bayes.MultinomialNB(), X_data_tfidf_ngram_char_svd, y_data, X_test_tfidf_ngram_char_svd, y_test, is_neuralnet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1edc3",
   "metadata": {},
   "source": [
    "### Dạng Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d6bb2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.8204976303317536\n",
      "Test accuracy:  0.8517459750263038\n"
     ]
    }
   ],
   "source": [
    "train_model(naive_bayes.BernoulliNB(), X_data_tfidf, y_data, X_test_tfidf, y_test, is_neuralnet=False)\n",
    "\n",
    "# kết quả thu được:\n",
    "# Train accuracy:  0.8485995457986374\n",
    "# Validation accuracy:  0.8293838862559242\n",
    "# Test accuracy:  0.8531554602664125\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c679877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.8024289099526066\n",
      "Test accuracy:  0.8149206916403628\n"
     ]
    }
   ],
   "source": [
    "train_model(naive_bayes.BernoulliNB(), X_data_tfidf_svd, y_data, X_test_tfidf_svd, y_test, is_neuralnet=False)\n",
    "\n",
    "# kết quả thu được:\n",
    "# Train accuracy:  0.8087746437152354\n",
    "# Validation accuracy:  0.8033175355450237\n",
    "# Test accuracy:  0.8143449864014453\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3bc070",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fab8c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.9129146919431279\n",
      "Test accuracy:  0.9186865979790761\n"
     ]
    }
   ],
   "source": [
    "train_model(svm.SVC(), X_data_tfidf_svd, y_data, X_test_tfidf_svd, y_test, is_neuralnet=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
